
# SuperVent Performance Optimization Plan

## Current State
- Python-based event generator
- ~35K events/second on c6gn.16xlarge
- Multiple configuration formats
- Complex dependency chains

## Target State
- 10M+ events/second baseline
- 50M+ events/second peak
- Sub-millisecond latency
- Full NIC saturation (15 × 25 Gbps)

## Architecture Design

### 1. Core Event Structure
```rust
// 16-byte aligned for optimal memory access
struct RawEvent {
    timestamp: u64,     // Nanoseconds since epoch
    method_idx: u8,     // Index into static method array
    path_idx: u8,       // Index into static path array
    status: u16,        // HTTP status code
    bytes: u32,         // Response size
}

// Static lookup tables
static METHODS: [&str; 4] = ["GET", "POST", "PUT", "DELETE"];
static PATHS: [&str; 20] = ["/api/v1/users", ...];
```

### 2. Memory Management
- Pre-allocated memory pools (2GB per generator thread)
- Huge pages for TLB efficiency
- Ring buffers between components
- Zero-copy until HTTP serialization
- Direct NIC buffer access

### 3. Threading Model
```rust
// Generator threads (one per core)
struct Generator {
    thread_id: u32,
    event_pool: Arc<EventPool>,
    output_buffer: Arc<RingBuffer>,
}

// Upload threads (one per NIC)
struct Uploader {
    nic_id: u32,
    input_buffer: Arc<RingBuffer>,
    http_client: AxiomClient,
}
```

### 4. Network Architecture
- Dedicated thread per NIC
- TCP optimizations:
  - TCP_NODELAY
  - TCP_QUICKACK
  - SO_REUSEPORT
- Jumbo frames (9000 MTU)
- Keep-alive connections
- Batch compression

### 5. Performance Features
1. Zero Allocation Path
   - Pre-allocated event pools
   - Static string references
   - Reusable buffers

2. SIMD Operations
   - Batch timestamp generation
   - Bulk event serialization
   - Vectorized random number generation

3. Network Optimization
   - Direct NIC binding
   - Kernel bypass (optional)
   - Zero-copy networking

4. Monitoring
   - Real-time throughput metrics
   - Latency histograms
   - Buffer utilization
   - NIC statistics

## Implementation Phases

### Phase 1: Core Engine (Week 1)
1. Basic Rust project setup
2. Event generation core
3. Memory pool implementation
4. Initial benchmarks

### Phase 2: Networking (Week 1-2)
1. Multi-NIC support
2. HTTP client optimization
3. Buffer management
4. Network metrics

### Phase 3: Configuration (Week 2)
1. YAML config parser
2. Runtime controls
3. Dynamic rate limiting
4. Monitoring dashboard

### Phase 4: Advanced Features (Week 3+)
1. Multi-dataset support
2. Complex event patterns
3. Dependency chains
4. Advanced monitoring

## Configuration Example
```yaml
runtime:
  threads: 64
  batch_size: 1_000_000
  buffer_size: 2_147_483_648  # 2GB per thread
  
network:
  interfaces: 15
  tcp_nodelay: true
  jumbo_frames: true
  keep_alive: true
  
monitoring:
  metrics_port: 9100
  detailed_stats: true
  
generation:
  target_rate: "unlimited"
  distribution: "random"
  timestamp_spread: "1h"
```

## Performance Goals
1. Event Generation
   - 1M events/second/core
   - 64M events/second total capacity
   - Sub-microsecond generation time

2. Network Utilization
   - 15 NICs × 25 Gbps = 375 Gbps theoretical
   - 80%+ NIC utilization
   - <1ms average latency

3. Resource Usage
   - 80% CPU utilization
   - 50% memory bandwidth
   - Zero heap allocation during generation

## Monitoring and Metrics
```rust
struct Metrics {
    events_generated: AtomicU64,
    events_sent: AtomicU64,
    buffer_utilization: AtomicF64,
    nic_throughput: [AtomicU64; 15],
    latency_histogram: Histogram,
}
```

## Next Steps
1. Create basic Rust project
2. Implement core event generation
3. Add network layer
4. Build monitoring
5. Optimize and benchmark

## Future Considerations
1. GPU acceleration for event generation
2. DPDK for network bypass
3. Custom memory allocator
4. Advanced event patterns