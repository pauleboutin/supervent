# Supervent Performance Optimization Plan

## Migration Overview
Moving from Python to Rust for:
- 100-1000x faster event generation
- Direct hardware access
- Zero-cost abstractions
- Predictable performance

## Current State
- Python implementation limitations:
  - GIL bottlenecks
  - High memory allocation overhead
  - String processing inefficiency
  - Network library overhead
- ~35K events/second on c6gn.16xlarge
- Multiple configuration formats
- Complex dependency chains

## Target State
- Complete rewrite in Rust
- 10M+ events/second baseline
- 50M+ events/second peak
- Sub-millisecond latency
- Full NIC saturation (15 × 25 Gbps)

## Why Rust?
1. Performance
   - Zero-cost abstractions
   - No garbage collection
   - LLVM optimizations
   - Direct hardware access

2. Safety
   - Memory safety without runtime cost
   - Thread safety guarantees
   - No null pointer exceptions

3. Modern Features
   - Async/await
   - Zero-copy parsing
   - SIMD support
   - Excellent tooling

## Architecture Design

### 1. Core Event Structure
```rust
// 16-byte aligned for optimal memory access
struct RawEvent {
    timestamp: u64,     // Nanoseconds since epoch
    method_idx: u8,     // Index into static method array
    path_idx: u8,       // Index into static path array
    status: u16,        // HTTP status code
    bytes: u32,         // Response size
}

// Static lookup tables
static METHODS: [&str; 4] = ["GET", "POST", "PUT", "DELETE"];
static PATHS: [&str; 20] = ["/api/v1/users", ...];
```

### 2. Memory Management
- Pre-allocated memory pools (2GB per generator thread)
- Huge pages for TLB efficiency
- Ring buffers between components
- Zero-copy until HTTP serialization
- Direct NIC buffer access

### 3. Threading Model
```rust
// Generator threads (one per core)
struct Generator {
    thread_id: u32,
    event_pool: Arc<EventPool>,
    output_buffer: Arc<RingBuffer>,
}

// Upload threads (one per NIC)
struct Uploader {
    nic_id: u32,
    input_buffer: Arc<RingBuffer>,
    http_client: AxiomClient,
}
```

### 4. Network Architecture
- Dedicated thread per NIC
- TCP optimizations:
  - TCP_NODELAY
  - TCP_QUICKACK
  - SO_REUSEPORT
- Jumbo frames (9000 MTU)
- Keep-alive connections
- Batch compression

### 5. Performance Features
1. Zero Allocation Path
   - Pre-allocated event pools
   - Static string references
   - Reusable buffers

2. SIMD Operations
   - Batch timestamp generation
   - Bulk event serialization
   - Vectorized random number generation

3. Network Optimization
   - Direct NIC binding
   - Kernel bypass (optional)
   - Zero-copy networking

4. Monitoring
   - Real-time throughput metrics
   - Latency histograms
   - Buffer utilization
   - NIC statistics

## Implementation Phases

### Phase 1: Core Engine (Week 1)
1. Basic Rust project setup
2. Event generation core
3. Memory pool implementation
4. Initial benchmarks

### Phase 2: Networking (Week 1-2)
1. Multi-NIC support
2. HTTP client optimization
3. Buffer management
4. Network metrics

### Phase 3: Configuration (Week 2)
1. YAML config parser
2. Runtime controls
3. Dynamic rate limiting
4. Monitoring dashboard

### Phase 4: Advanced Features (Week 3+)
1. Multi-dataset support
2. Complex event patterns
3. Dependency chains
4. Advanced monitoring

## Initial Prototype
Focus on raw speed with minimal config:
```rust
fn main() {
    // Pre-allocate 1GB event buffer
    let mut events = Vec::with_capacity(1_000_000);
    
    // One generator per core
    let generators: Vec<_> = (0..num_cpus::get())
        .map(|core_id| {
            thread::spawn(move || {
                // Pin to core
                core_affinity::set_for_current(core_id);
                generate_events()
            })
        })
        .collect();
    
    // One uploader per NIC
    let uploaders: Vec<_> = (0..15)
        .map(|nic_id| {
            thread::spawn(move || {
                upload_events(nic_id)
            })
        })
        .collect();
}
```

Goals:
1. Validate raw performance (~1M events/sec/core)
2. Test NIC saturation
3. Measure memory bandwidth
4. Profile bottlenecks

## Configuration Example
```yaml
runtime:
  threads: 64
  batch_size: 1_000_000
  buffer_size: 2_147_483_648  # 2GB per thread
  
network:
  interfaces: 15
  tcp_nodelay: true
  jumbo_frames: true
  keep_alive: true
  
monitoring:
  metrics_port: 9100
  detailed_stats: true
  
generation:
  target_rate: "unlimited"
  distribution: "random"
  timestamp_spread: "1h"
```

## Performance Goals
1. Event Generation
   - 1M events/second/core
   - 64M events/second total capacity
   - Sub-microsecond generation time

2. Network Utilization
   - 15 NICs × 25 Gbps = 375 Gbps theoretical
   - 80%+ NIC utilization
   - <1ms average latency

3. Resource Usage
   - 80% CPU utilization
   - 50% memory bandwidth
   - Zero heap allocation during generation

## Monitoring and Metrics
```rust
struct Metrics {
    events_generated: AtomicU64,
    events_sent: AtomicU64,
    buffer_utilization: AtomicF64,
    nic_throughput: [AtomicU64; 15],
    latency_histogram: Histogram,
}
```

## Next Steps
1. Create basic Rust project
2. Implement core event generation
3. Add network layer
4. Build monitoring
5. Optimize and benchmark

## Future Considerations
1. GPU acceleration for event generation
2. DPDK for network bypass
3. Custom memory allocator
4. Advanced event patterns
